{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Logistic Regression with Regularization\n",
    "## COSC 4337 - Data Science II\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Introduction\n",
    "\n",
    "In this lab, we will build upon our knowledge of Logistic Regression by introducing **regularization**. Regularization is a technique used to prevent overfitting by penalizing large coefficients in the model. This helps create a more generalizable model that performs better on unseen data.\n",
    "\n",
    "We will explore two common types of regularization:\n",
    "- **L1 Regularization (Lasso Regression)**: Adds a penalty equal to the *absolute value* of the magnitude of coefficients. This can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "- **L2 Regularization (Ridge Regression)**: Adds a penalty equal to the *square* of the magnitude of coefficients. This shrinks coefficients towards zero but rarely sets them exactly to zero.\n",
    "\n",
    "We will use the **Wisconsin Breast Cancer dataset**, where the goal is to predict whether a tumor is malignant or benign based on various measurements.\n",
    "\n",
    "#### Dataset Variable Information\n",
    "| Feature             | Description                                    |\n",
    "|---------------------|------------------------------------------------|\n",
    "| **`target`** | Diagnosis (0 = malignant, 1 = benign)          |\n",
    "| `mean radius`       | Mean of distances from center to perimeter     |\n",
    "| `mean texture`      | Standard deviation of gray-scale values        |\n",
    "| `mean perimeter`    | Mean size of the core tumor                    |\n",
    "| `mean area`         | Mean area of the tumor                         |\n",
    "| `mean smoothness`   | Mean of local variation in radius lengths      |\n",
    "| ...                 | (And 25 other geometric features)              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Requirements\n",
    "\n",
    "For this assignment, we will use the breast cancer dataset available directly from the `scikit-learn` library. No external files are needed.\n",
    "\n",
    "***\n",
    "**NOTE**: Required actions you need to perform are marked with `***`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Initial Imports\n",
    "***\n",
    "*** Begin by importing the necessary packages for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Data Loading and Preparation\n",
    "***\n",
    "We'll load the dataset from `scikit-learn` and place it into a pandas DataFrame for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Create a DataFrame with features\n",
    "feats = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "\n",
    "# Create a DataFrame for the target variable\n",
    "target = pd.DataFrame(cancer.target, columns=['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Data Exploration\n",
    "***\n",
    "*** Take a look at the feature and target data using `.head()` to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) Train-Test Split\n",
    "***\n",
    "Now, we will split our data into training and testing sets. This ensures we evaluate our model on data it has never seen before. We will use 20% of the data for testing.\n",
    "\n",
    "We will also use **cross-validation** on the training set to find the best regularization hyperparameter. `LogisticRegressionCV` handles this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Let's verify the shapes of our new datasets to ensure the split was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (455, 30)\n",
      "Shape of y_train: (455, 1)\n",
      "Shape of X_test: (114, 30)\n",
      "Shape of y_test: (114, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the training and testing sets\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Model Training with Regularization and Cross-Validation\n",
    "***\n",
    "We will now train two Logistic Regression models using `LogisticRegressionCV`, which performs cross-validation to find the best regularization strength `C`.\n",
    "\n",
    "- **`model_l1`**: Uses L1 regularization (`penalty='l1'`). Requires a compatible solver like `'liblinear'`.\n",
    "- **`model_l2`**: Uses L2 regularization (`penalty='l2'`), which is the default.\n",
    "\n",
    "The parameter `Cs` defines the grid of values for `C` (which is the *inverse* of regularization strength, so smaller `C` means stronger regularization) that the cross-validation will test.\n",
    "\n",
    "*** Instantiate and fit both an L1 and an L2 regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isrtr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 10000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=10000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Test Accuracy: 0.9736842105263158\n",
      "L2 Test Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "# Define the grid of C values to test\n",
    "Cs = np.logspace(-4, 4, 10)\n",
    "\n",
    "# L1-regularized logistic regression\n",
    "model_l1 = LogisticRegressionCV(\n",
    "    Cs=Cs,\n",
    "    cv=5,\n",
    "    penalty='l1',\n",
    "    solver='liblinear',\n",
    "    scoring='accuracy',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "model_l1.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# L2-regularized logistic regression\n",
    "model_l2 = LogisticRegressionCV(\n",
    "    Cs=Cs,\n",
    "    cv=5,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    scoring='accuracy',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "model_l2.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate both Models:\n",
    "print(\"L1 Test Accuracy:\", model_l1.score(X_test, y_test))\n",
    "print(\"L2 Test Accuracy:\", model_l2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1) Optimal Hyperparameters\n",
    "***\n",
    "The `LogisticRegressionCV` object automatically stores the best hyperparameter `C` found during cross-validation.\n",
    "\n",
    "*** Print the best value of `C` for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C for L1: 21.54434690031882\n",
      "Best C for L2: 166.81005372000558\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "print(\"Best C for L1:\", model_l1.C_[0])\n",
    "print(\"Best C for L2:\", model_l2.C_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Model Evaluation\n",
    "***\n",
    "Now we'll use our trained models to make predictions on the held-out test set (`X_test`) and compare them to the true labels (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "# Make predictions using both models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1) Evaluation Metrics\n",
    "***\n",
    "Let's calculate the key evaluation metrics for both models to see how they performed.\n",
    "\n",
    "*** Print the accuracy, precision, recall, and f1-score for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Feature Importances\n",
    "***\n",
    "A key difference between L1 and L2 regularization is their effect on the model's coefficients. Let's examine them.\n",
    "\n",
    "- **L1** tends to produce **sparse** coefficients, driving the weights of less important features to exactly zero.\n",
    "- **L2** penalizes large coefficients but only shrinks them towards zero, rarely making them exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1) L1 Model Coefficients\n",
    "***\n",
    "*** Create a DataFrame to view the feature names and their corresponding coefficients from the L1 model. Notice how many have become zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2) L2 Model Coefficients\n",
    "***\n",
    "*** Now, do the same for the L2 model. Observe that while many coefficients are small, none (or very few) are exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Conclusion\n",
    "***\n",
    "In this lab, we have seen how to create logistic regression models that include regularization. We used cross-validation to tune the regularization hyperparameter `C` and compared the performance of L1 and L2 penalties.\n",
    "\n",
    "Regularization is a crucial technique for building robust machine learning models that generalize well to new data by preventing overfitting. We also observed the feature-selecting property of L1 regularization, which can be very useful for models with a large number of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

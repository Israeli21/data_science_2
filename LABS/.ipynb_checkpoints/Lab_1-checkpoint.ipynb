{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Logistic Regression with Regularization\n",
    "## COSC 4337 - Data Science II\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Introduction\n",
    "\n",
    "In this lab, we will build upon our knowledge of Logistic Regression by introducing **regularization**. Regularization is a technique used to prevent overfitting by penalizing large coefficients in the model. This helps create a more generalizable model that performs better on unseen data.\n",
    "\n",
    "We will explore two common types of regularization:\n",
    "- **L1 Regularization (Lasso Regression)**: Adds a penalty equal to the *absolute value* of the magnitude of coefficients. This can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "- **L2 Regularization (Ridge Regression)**: Adds a penalty equal to the *square* of the magnitude of coefficients. This shrinks coefficients towards zero but rarely sets them exactly to zero.\n",
    "\n",
    "We will use the **Wisconsin Breast Cancer dataset**, where the goal is to predict whether a tumor is malignant or benign based on various measurements.\n",
    "\n",
    "#### Dataset Variable Information\n",
    "| Feature             | Description                                    |\n",
    "|---------------------|------------------------------------------------|\n",
    "| **`target`** | Diagnosis (0 = malignant, 1 = benign)          |\n",
    "| `mean radius`       | Mean of distances from center to perimeter     |\n",
    "| `mean texture`      | Standard deviation of gray-scale values        |\n",
    "| `mean perimeter`    | Mean size of the core tumor                    |\n",
    "| `mean area`         | Mean area of the tumor                         |\n",
    "| `mean smoothness`   | Mean of local variation in radius lengths      |\n",
    "| ...                 | (And 25 other geometric features)              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Requirements\n",
    "\n",
    "For this assignment, we will use the breast cancer dataset available directly from the `scikit-learn` library. No external files are needed.\n",
    "\n",
    "***\n",
    "**NOTE**: Required actions you need to perform are marked with `***`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Initial Imports\n",
    "***\n",
    "*** Begin by importing the necessary packages for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Data Loading and Preparation\n",
    "***\n",
    "We'll load the dataset from `scikit-learn` and place it into a pandas DataFrame for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Create a DataFrame with features\n",
    "feats = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "\n",
    "# Create a DataFrame for the target variable\n",
    "target = pd.DataFrame(cancer.target, columns=['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Data Exploration\n",
    "***\n",
    "*** Take a look at the feature and target data using `.head()` to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) Train-Test Split\n",
    "***\n",
    "Now, we will split our data into training and testing sets. This ensures we evaluate our model on data it has never seen before. We will use 20% of the data for testing.\n",
    "\n",
    "We will also use **cross-validation** on the training set to find the best regularization hyperparameter. `LogisticRegressionCV` handles this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Let's verify the shapes of our new datasets to ensure the split was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (455, 30)\n",
      "Shape of y_train: (455, 1)\n",
      "Shape of X_test: (114, 30)\n",
      "Shape of y_test: (114, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the training and testing sets\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Model Training with Regularization and Cross-Validation\n",
    "***\n",
    "We will now train two Logistic Regression models using `LogisticRegressionCV`, which performs cross-validation to find the best regularization strength `C`.\n",
    "\n",
    "- **`model_l1`**: Uses L1 regularization (`penalty='l1'`). Requires a compatible solver like `'liblinear'`.\n",
    "- **`model_l2`**: Uses L2 regularization (`penalty='l2'`), which is the default.\n",
    "\n",
    "The parameter `Cs` defines the grid of values for `C` (which is the *inverse* of regularization strength, so smaller `C` means stronger regularization) that the cross-validation will test.\n",
    "\n",
    "*** Instantiate and fit both an L1 and an L2 regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isrtr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 10000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=10000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Test Accuracy: 0.9736842105263158\n",
      "L2 Test Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "# Define the grid of C values to test\n",
    "Cs = np.logspace(-4, 4, 10)\n",
    "\n",
    "# L1-regularized logistic regression\n",
    "model_l1 = LogisticRegressionCV(\n",
    "    Cs=Cs,\n",
    "    cv=5,\n",
    "    penalty='l1',\n",
    "    solver='liblinear',\n",
    "    scoring='accuracy',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "model_l1.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# L2-regularized logistic regression\n",
    "model_l2 = LogisticRegressionCV(\n",
    "    Cs=Cs,\n",
    "    cv=5,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    scoring='accuracy',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "model_l2.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate both Models:\n",
    "print(\"L1 Test Accuracy:\", model_l1.score(X_test, y_test))\n",
    "print(\"L2 Test Accuracy:\", model_l2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1) Optimal Hyperparameters\n",
    "***\n",
    "The `LogisticRegressionCV` object automatically stores the best hyperparameter `C` found during cross-validation.\n",
    "\n",
    "*** Print the best value of `C` for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C for L1: 21.54434690031882\n",
      "Best C for L2: 166.81005372000558\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "print(\"Best C for L1:\", model_l1.C_[0])\n",
    "print(\"Best C for L2:\", model_l2.C_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Model Evaluation\n",
    "***\n",
    "Now we'll use our trained models to make predictions on the held-out test set (`X_test`) and compare them to the true labels (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Predictions: [1 0 0 1 1 0 0 0 1 1]\n",
      "L2 Predictions: [1 0 0 1 1 0 0 0 1 1]\n",
      "True Labels: [1 0 0 1 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "# Make predictions using both models\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "\n",
    "print(\"L1 Predictions:\", y_pred_l1[:10])\n",
    "print(\"L2 Predictions:\", y_pred_l2[:10])\n",
    "print(\"True Labels:\", y_test[:10].values.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1) Evaluation Metrics\n",
    "***\n",
    "Let's calculate the key evaluation metrics for both models to see how they performed.\n",
    "\n",
    "*** Print the accuracy, precision, recall, and f1-score for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Accuracy: 0.9736842105263158\n",
      "L1 Precision: 0.9722222222222222\n",
      "L1 Recall: 0.9859154929577465\n",
      "L1 F1-Score: 0.9790209790209791\n",
      "\n",
      "L2 Accuracy: 0.9736842105263158\n",
      "L2 Precision: 0.9722222222222222\n",
      "L2 Recall: 0.9859154929577465\n",
      "L2 F1-Score: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# L1 metrics\n",
    "print(\"L1 Accuracy:\", accuracy_score(y_test, y_pred_l1))\n",
    "print(\"L1 Precision:\", precision_score(y_test, y_pred_l1))\n",
    "print(\"L1 Recall:\", recall_score(y_test, y_pred_l1))\n",
    "print(\"L1 F1-Score:\", f1_score(y_test, y_pred_l1))\n",
    "\n",
    "# L2 metrics\n",
    "print(\"\\nL2 Accuracy:\", accuracy_score(y_test, y_pred_l2))\n",
    "print(\"L2 Precision:\", precision_score(y_test, y_pred_l2))\n",
    "print(\"L2 Recall:\", recall_score(y_test, y_pred_l2))\n",
    "print(\"L2 F1-Score:\", f1_score(y_test, y_pred_l2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Feature Importances\n",
    "***\n",
    "A key difference between L1 and L2 regularization is their effect on the model's coefficients. Let's examine them.\n",
    "\n",
    "- **L1** tends to produce **sparse** coefficients, driving the weights of less important features to exactly zero.\n",
    "- **L2** penalizes large coefficients but only shrinks them towards zero, rarely making them exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1) L1 Model Coefficients\n",
    "***\n",
    "Create a DataFrame to view the feature names and their corresponding coefficients from the L1 model. Notice how many have become zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Coefficient  AbsCoefficient\n",
      "7       mean concave points   -49.230893       49.230893\n",
      "27     worst concave points   -23.385731       23.385731\n",
      "16          concavity error    21.735155       21.735155\n",
      "24         worst smoothness   -12.375647       12.375647\n",
      "28           worst symmetry   -10.118864       10.118864\n",
      "26          worst concavity    -8.225833        8.225833\n",
      "25        worst compactness     5.631436        5.631436\n",
      "11            texture error     2.818668        2.818668\n",
      "0               mean radius     0.877476        0.877476\n",
      "12          perimeter error    -0.725487        0.725487\n",
      "6            mean concavity    -0.586399        0.586399\n",
      "21            worst texture    -0.543007        0.543007\n",
      "20             worst radius     0.471596        0.471596\n",
      "22          worst perimeter     0.114109        0.114109\n",
      "13               area error    -0.109679        0.109679\n",
      "2            mean perimeter     0.095212        0.095212\n",
      "1              mean texture     0.082716        0.082716\n",
      "23               worst area    -0.023241        0.023241\n",
      "3                 mean area    -0.011576        0.011576\n",
      "15        compactness error     0.000000        0.000000\n",
      "19  fractal dimension error     0.000000        0.000000\n",
      "18           symmetry error     0.000000        0.000000\n",
      "17     concave points error     0.000000        0.000000\n",
      "14         smoothness error     0.000000        0.000000\n",
      "10             radius error     0.000000        0.000000\n",
      "9    mean fractal dimension     0.000000        0.000000\n",
      "8             mean symmetry     0.000000        0.000000\n",
      "5          mean compactness     0.000000        0.000000\n",
      "4           mean smoothness     0.000000        0.000000\n",
      "29  worst fractal dimension     0.000000        0.000000\n",
      "\n",
      "Number of zero coefficients (L1): 11 out of 30\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "coef_l1 = model_l1.coef_[0]   # coefficients (array)\n",
    "features = feats.columns      # feature names\n",
    "\n",
    "# Put into DataFrame\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': coef_l1\n",
    "})\n",
    "\n",
    "# Sort by absolute value of coefficient (largest first)\n",
    "coef_df['AbsCoefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='AbsCoefficient', ascending=False)\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "# Count how many coefficients are exactly zero\n",
    "num_zero = (coef_df['Coefficient'] == 0).sum()\n",
    "print(f\"\\nNumber of zero coefficients (L1): {num_zero} out of {len(coef_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2) L2 Model Coefficients\n",
    "Now, do the same for the L2 model. Observe that while many coefficients are small, none (or very few) are exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Coefficient  AbsCoefficient\n",
      "27     worst concave points   -14.859037       14.859037\n",
      "24         worst smoothness   -13.324789       13.324789\n",
      "7       mean concave points   -11.290107       11.290107\n",
      "28           worst symmetry   -10.487217       10.487217\n",
      "16          concavity error     9.764676        9.764676\n",
      "15        compactness error     8.525845        8.525845\n",
      "6            mean concavity    -7.698148        7.698148\n",
      "26          worst concavity    -7.502621        7.502621\n",
      "25        worst compactness     6.296334        6.296334\n",
      "4           mean smoothness    -5.601605        5.601605\n",
      "11            texture error     3.405107        3.405107\n",
      "5          mean compactness     2.689318        2.689318\n",
      "0               mean radius     1.909857        1.909857\n",
      "18           symmetry error     1.722300        1.722300\n",
      "14         smoothness error    -1.699095        1.699095\n",
      "19  fractal dimension error     1.487678        1.487678\n",
      "29  worst fractal dimension     1.310582        1.310582\n",
      "8             mean symmetry    -1.272524        1.272524\n",
      "10             radius error    -0.956308        0.956308\n",
      "17     concave points error    -0.907028        0.907028\n",
      "12          perimeter error    -0.885924        0.885924\n",
      "21            worst texture    -0.665552        0.665552\n",
      "20             worst radius    -0.596435        0.596435\n",
      "2            mean perimeter    -0.399013        0.399013\n",
      "9    mean fractal dimension     0.352699        0.352699\n",
      "1              mean texture     0.221714        0.221714\n",
      "22          worst perimeter     0.176316        0.176316\n",
      "13               area error    -0.099088        0.099088\n",
      "23               worst area    -0.015975        0.015975\n",
      "3                 mean area     0.010413        0.010413\n",
      "\n",
      "Number of zero coefficients (L2): 0 out of 30\n"
     ]
    }
   ],
   "source": [
    "# *** YOUR CODE HERE ***\n",
    "coef_l2 = model_l2.coef_[0]\n",
    "features = feats.columns\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': coef_l2\n",
    "})\n",
    "\n",
    "coef_df['AbsCoefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='AbsCoefficient', ascending=False)\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "num_zero = (coef_df['Coefficient'] == 0).sum()\n",
    "print(f\"\\nNumber of zero coefficients (L2): {num_zero} out of {len(coef_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Conclusion\n",
    "***\n",
    "In this lab, we have seen how to create logistic regression models that include regularization. We used cross-validation to tune the regularization hyperparameter `C` and compared the performance of L1 and L2 penalties.\n",
    "\n",
    "Regularization is a crucial technique for building robust machine learning models that generalize well to new data by preventing overfitting. We also observed the feature-selecting property of L1 regularization, which can be very useful for models with a large number of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

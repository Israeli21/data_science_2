{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd58bdb-3f05-4358-9045-e5be876ec0e6",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "#### September 5, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e233933-4fb2-462c-a9f9-6f7066c8d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter W1        b         Output    Loss      \n",
      "1    0.518     0.018     0.622     0.143     \n",
      "2    0.535     0.035     0.631     0.136     \n",
      "3    0.552     0.052     0.639     0.131     \n",
      "4    0.568     0.068     0.646     0.125     \n",
      "5    0.583     0.083     0.654     0.120     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isrtr\\AppData\\Local\\Temp\\ipykernel_13512\\73830287.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"{i:<5}{float(W[0]):<10.3f}{float(b):<10.3f}{float(a[0]):<10.3f}{float(loss[0]):<10.3f}\")\n"
     ]
    }
   ],
   "source": [
    "# In class 09/02\n",
    "# September 3, 2025\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid + derivative\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def sigmoid_deriv(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# Data\n",
    "X = np.array([[1, 0]])   # input (x1=1, x2=0)\n",
    "y = np.array([1])        # target\n",
    "\n",
    "# Initialize params\n",
    "W = np.array([0.5, -0.5])   # [W1, W2]\n",
    "b = 0.0\n",
    "lr = 0.1\n",
    "\n",
    "print(f\"{'Iter':<5}{'W1':<10}{'b':<10}{'Output':<10}{'Loss':<10}\")\n",
    "for i in range(1, 6):  # run 5 iterations\n",
    "    # Forward\n",
    "    z = np.dot(X, W) + b\n",
    "    a = sigmoid(z)\n",
    "    loss = (y - a)**2\n",
    "    \n",
    "    # Backward\n",
    "    dL_da = -2*(y - a)\n",
    "    da_dz = a*(1-a)\n",
    "    dL_dz = dL_da * da_dz\n",
    "    dW = X.T * dL_dz\n",
    "    db = dL_dz\n",
    "    \n",
    "    # Update\n",
    "    W -= lr * dW.flatten()\n",
    "    b -= lr * db\n",
    "    \n",
    "    # Convert numpy values to float before formatting\n",
    "    print(f\"{i:<5}{float(W[0]):<10.3f}{float(b):<10.3f}{float(a[0]):<10.3f}{float(loss[0]):<10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49651936-bf40-46bb-af80-cbe88f05b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2558\n",
      "Epoch 1000, Loss: 0.2494\n",
      "Epoch 2000, Loss: 0.2454\n",
      "Epoch 3000, Loss: 0.2047\n",
      "Epoch 4000, Loss: 0.1532\n",
      "Epoch 5000, Loss: 0.1387\n",
      "Epoch 6000, Loss: 0.1336\n",
      "Epoch 7000, Loss: 0.1312\n",
      "Epoch 8000, Loss: 0.1297\n",
      "Epoch 9000, Loss: 0.1288\n",
      "\n",
      "Trained output:\n",
      "[[0.053]\n",
      " [0.496]\n",
      " [0.951]\n",
      " [0.503]]\n"
     ]
    }
   ],
   "source": [
    "# XOR Input and Output\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10000\n",
    "lr = 0.1\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights and biases\n",
    "wh = np.random.randn(2, 2)   # weights input -> hidden\n",
    "bh = np.zeros((1, 2))        # bias hidden\n",
    "wout = np.random.randn(2, 1) # weights hidden -> output\n",
    "bout = np.zeros((1, 1))      # bias output\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # ---- Forward Propagation ----\n",
    "    hidden_input = np.dot(X, wh) + bh\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    final_input = np.dot(hidden_output, wout) + bout\n",
    "    final_output = sigmoid(final_input)\n",
    "\n",
    "    # ---- Error ----\n",
    "    error = y - final_output\n",
    "\n",
    "    # ---- Backpropagation ----\n",
    "    d_output = error * sigmoid_derivative(final_output)\n",
    "\n",
    "    error_hidden = d_output.dot(wout.T)\n",
    "    d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # ---- Update weights and biases ----\n",
    "    wout += hidden_output.T.dot(d_output) * lr\n",
    "    bout += np.sum(d_output, axis=0, keepdims=True) * lr\n",
    "    wh += X.T.dot(d_hidden) * lr\n",
    "    bh += np.sum(d_hidden, axis=0, keepdims=True) * lr\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Final output\n",
    "print(\"\\nTrained output:\")\n",
    "print(final_output.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
